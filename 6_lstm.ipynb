{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Machine Learning and Deep Learning with Tensorflow\n",
    "=============\n",
    "\n",
    "Long Short Term Memory (LSTM) RNN model\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is 27:\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(\"vocabulary size is %d:\" %vocab_size)\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ' or char == None:\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0 and dictid < 27:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "              return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vSize):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vSize], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution(vSize):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vSize])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:   \n",
    "    # input: input gate (ix), forget gate (fx), memory cell (cx), output gate (ox)\n",
    "    X_ifco = tf.Variable(tf.truncated_normal([vocab_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # previous output: input gate (im), forget gate (fm), memory cell (cm), output gate (om)\n",
    "    M_ifco = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    # bias: input gate (ib), forget gate (if), memory cell (cb), output gate (ob)\n",
    "    B_ifco = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        \"\"\"\n",
    "        all_matmul =  tf.matmul(i, X_ifco) + tf.matmul(o, M_ifco) + B_ifco\n",
    "        input_matmul, forget_matmul, update, output_matmul = tf.split(1, 4, all_matmul)\n",
    "        all_gates = tf.sigmoid(all_matmul)\n",
    "        input_gate, forget_gate, _, output_gate = tf.split(1, 4, all_gates)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocab_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocab_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def graphExec(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels))))\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution(vocab_size), vocab_size)\n",
    "                        sentence = characters(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction, vocab_size)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298486 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "usvr  osqy  oirfbqvfnl hchrth iis hefuse aslkvwxrmxn tcvmsgorzmtu yueqjltrmvhrau\n",
      "bltaozqqrcp yaqeb d qprgvkltojumvbvfbfy hoeejgepzvt o gwdiosueuanhilvetttjbiesal\n",
      "owiwaylxukcbvg r aspyrpepwzksqxsiskn vrhahlriketowrurjslpantiqezcvnjekzpaira  e \n",
      "yawb psgpy lzqauc fz toq  to re rx ecqz la tiwadnvemp eefltante xcjt k wnpeljbpc\n",
      "gz jkrrgxs xvehw wcnzizty  rgjk ynvnbzsongzyxhucieszegvnic  xnd a dlhktmkfevatir\n",
      "================================================================================\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 100: 2.599799 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.13\n",
      "Validation set perplexity: 10.81\n",
      "Average loss at step 200: 2.253537 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 300: 2.099551 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 400: 1.998167 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 500: 1.937039 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 600: 1.910315 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.853282 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 800: 1.817695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 900: 1.829413 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1000: 1.823902 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "================================================================================\n",
      "king afucisp attle soffray compred by tover servaling sho the beer compuans one \n",
      "yow in a seed vistorate duinagh deriscopthirs riccramose the reath his k hamier \n",
      "gens most to the unferver the deary unry radnion is weathear berty dilled in to \n",
      "word iban chirtucary c sopoct diles akwed tran cicne to cli nomay yeartracsix an\n",
      "bate forksting id bevicmlovudig in eovaduicariss the seevers one hivh c they an \n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.775798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1200: 1.754837 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.730460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1400: 1.750088 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.740615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.747843 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1700: 1.710959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.677731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1900: 1.646796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.695079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "reast and tosiis election toge proper tin one nine nine four the follown for his\n",
      "fite st noter agens the can  ergenibualt sot evol they mudiom such it creal seko\n",
      "fict an exgerning to the crainmines blannad and for accust supprong thou propras\n",
      "tberrian to internation officious or popn this six feady hefdinish concent liffu\n",
      "pinctions contion conoftions anceslexp was inalle eng goali ia explibeal est to \n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.687011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2200: 1.681738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2300: 1.638839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2400: 1.660796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.678707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2600: 1.651518 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2700: 1.654980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.650776 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.648818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3000: 1.647464 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "why clession one nine six jedse recoldoing irenany has one in ne was of a collfa\n",
      "won ankestiveral contaous theyguging a the lassed the cobiting sharal was in her\n",
      "y the espaced in experiency seven eight one zero councifisu and solity in a ribe\n",
      "x ages to archyings inted britate how is wherea the become damed one theoply ing\n",
      "gual hadlm up time decand to weinny and mind throut one four de the inducatives \n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.632161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3200: 1.645097 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.640411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.671678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3500: 1.654933 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3600: 1.664794 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.650836 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.640213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.641982 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.648107 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "quiter suspervac poneperally senting in alrelmais archoress in a press roals azg\n",
      "e these the world including us by she dealis seven fivenswing pardical enrismany\n",
      "fols way on the ellangetbs hage add adlantly abromesse educal have when of reduc\n",
      "ilm not marsome of the has bassing the compleotaki and lost forfall sorn bitjest\n",
      "atticate of the canjusion au water game pelibed to fils the tombalspopble or int\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100: 1.631798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.633377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4300: 1.613636 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4400: 1.609051 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.616882 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.615025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4700: 1.622855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4800: 1.633212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4900: 1.632430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5000: 1.608292 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "exons seev bssidic that the was will propesses not five two five brag as ono of \n",
      "est produc in mille ckncaarism skarg while the play breminioncicin resplistics f\n",
      "nks govern on disliving bavol goldinific maugst of not the profiction mimin mef \n",
      "knoth of sladic mory the countimed reab s war eno portacties maintrain profumifi\n",
      "king take linten ancest lik sea twentury extantes pielt were is have sange publi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100: 1.604061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.589603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5300: 1.580704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400: 1.577647 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5500: 1.565895 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5600: 1.582594 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5700: 1.571373 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5800: 1.581856 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.574140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6000: 1.548082 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "f actisive in the rofetall masside company s sothon miggly granom tytery rinctor\n",
      "poccans referent about bleane officially andsibit amperors its prebibits and pet\n",
      "ching no schotion and will homan tr and rules lamicaging states he forchere with\n",
      "pp made the law gubok flows mient and charred juse on notay v schoal orizonst hi\n",
      "h in a his hars holt powerls other ghalosa semains way and lalt languary entiref\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6100: 1.563090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.539599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.542349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.542346 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6500: 1.560011 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.595723 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.578300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.605067 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.580907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.579284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      " imbroto drugi and is imburk know governmeno studies idented letter and used and\n",
      "n modernioous in a popular to this followed inald descripon red provider in one \n",
      "re approper were correactifian while city if a like recentle dotoky the complex \n",
      "yli sangoust brougp kfnearoment cainally eschaaised made were ore thingd genefog\n",
      "an many by the mallikercal fourdated re cult a congutily and the been albectivel\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "CPU times: user 4min 49s, sys: 41.9 s, total: 5min 31s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%time graphExec(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "TODO 1\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "ngram = 2\n",
    "\n",
    "class nGramBatchGenerator(object):\n",
    "    def __init__(self, ngram, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self.ngram = ngram\n",
    "        self.vocab_size = int(math.pow(vocab_size, ngram))\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)] # vector of the batch_size cursors positions in text\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = list()\n",
    "        for b in range(self._batch_size):\n",
    "            # getting the ids for ngram chars\n",
    "            j = 0\n",
    "            nCharId = 0\n",
    "            for i in range(self.ngram):\n",
    "                pos = self._cursor[b] + i\n",
    "                char = self._text[pos] if pos < self._text_size else ' '\n",
    "                charId = char2id(char)\n",
    "                if (charId == 0): continue\n",
    "                else: \n",
    "                    nCharId = nCharId + charId*int(math.pow(vocab_size, ngram-i-1))\n",
    "                    j = j + 1\n",
    "            batch.append(nCharId)\n",
    "            self._cursor[b] = (self._cursor[b] + self.ngram - 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones. \"\"\" \n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def nGramCharacters(probabilities, ngram):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [formatCharacter(c, ngram)\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def formatCharacter(c, ngram): \n",
    "    formatString = ''\n",
    "    args = []\n",
    "    for i in range(ngram):\n",
    "        idn = (c//int(math.pow(vocab_size, ngram-i-1))) % vocab_size\n",
    "        args.append(id2char(idn))\n",
    "        formatString = formatString + '{%d}'%i\n",
    "    return formatString.format(*args)\n",
    "\n",
    "\n",
    "train_batches = nGramBatchGenerator(ngram, train_text, batch_size, num_unrollings)\n",
    "valid_batches = nGramBatchGenerator(ngram, valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "n_vocab_size = int(math.pow(vocab_size, ngram))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:   \n",
    "    # input: input gate (ix), forget gate (fx), memory cell (cx), output gate (ox)\n",
    "    X_ifco = tf.Variable(tf.truncated_normal([n_vocab_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # previous output: input gate (im), forget gate (fm), memory cell (cm), output gate (om)\n",
    "    M_ifco = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    # bias: input gate (ib), forget gate (if), memory cell (cb), output gate (ob)\n",
    "    B_ifco = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, n_vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([n_vocab_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        \"\"\"\n",
    "        all_matmul = tf.nn.embedding_lookup(X_ifco, i) + tf.matmul(o, M_ifco) + B_ifco\n",
    "        input_matmul, forget_matmul, update, output_matmul = tf.split(1, 4, all_matmul)\n",
    "        all_gates = tf.sigmoid(all_matmul)\n",
    "        input_gate, forget_gate, _, output_gate = tf.split(1, 4, all_gates)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.int64, \n",
    "                         shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                            saved_sample_output, \n",
    "                                            saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def graphExec(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run([optimizer, loss, train_prediction, \n",
    "                                                 learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                # create sparse embedding one-hot encoding\n",
    "                embeddedLabels = np.zeros(predictions.shape)\n",
    "                for i, j in enumerate(labels):\n",
    "                    embeddedLabels[i, j] = 1.0   \n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, embeddedLabels))))\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution(n_vocab_size), n_vocab_size)\n",
    "                        sentence = characters(feed)[0]\n",
    "                        feed = [np.argmax(feed)]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction, n_vocab_size)\n",
    "                            sentence += characters(feed)[0]\n",
    "                            feed = [np.argmax(feed)]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    labels = np.zeros((1, n_vocab_size))\n",
    "                    labels[0, b[1]] = 1.0\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591617 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.96\n",
      "================================================================================\n",
      "        o                    x         n                                        \n",
      "             o                                       x                   n      \n",
      "   v                        m    i                               n              \n",
      "               nh                                                  q    m       \n",
      "                                                                                \n",
      "================================================================================\n",
      "Validation set perplexity: 668.90\n",
      "Average loss at step 100: 5.352952 learning rate: 10.000000\n",
      "Minibatch perplexity: 118.92\n",
      "Validation set perplexity: 115.87\n",
      "Average loss at step 200: 4.101295 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.23\n",
      "Validation set perplexity: 28.94\n",
      "Average loss at step 300: 2.896761 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.17\n",
      "Validation set perplexity: 13.18\n",
      "Average loss at step 400: 2.395813 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 9.90\n",
      "Average loss at step 500: 2.185556 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.41\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 600: 2.033896 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 700: 1.959520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 800: 1.935834 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 900: 1.902237 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 1000: 1.891421 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "   o  a    h       h   g         i  h       r       g      s       s         t  \n",
      "  o   s              d       s           f      t   u     c     c      e       o\n",
      "  t       n          s     n    n   t     l        w     o  t   z    z    g     \n",
      "    t   h      d     t  h  a      h      i    t    t    w    s       h   f      \n",
      "   s            v      r         i  t   s      i  t   p      n   m       b  a   \n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1100: 1.837027 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 1200: 1.788823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1300: 1.777692 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1400: 1.769283 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.752961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.733586 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1700: 1.720734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.688791 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1900: 1.687250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.673053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "       o  s      o  i   b o  s       i       i           h  c          o  w     \n",
      "     i  i  i      q     s        t     s b      t  d        o  t   a        i  p\n",
      "        s    f   c      t  c     i  n        a    i       b   t  p      w   b   \n",
      "  n b           a s     o  h   p       a    a    g   o  t   m        w   t   b  \n",
      "        o      m              b   n     b t    o  n    a w      a l    i  t   s \n",
      "================================================================================\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2100: 1.676949 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2200: 1.696979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2300: 1.689338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2400: 1.672994 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2500: 1.675588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2600: 1.653111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2700: 1.661968 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2800: 1.653303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2900: 1.650166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3000: 1.661444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "   b    o     l     b    b t     s    o  i  w    x n   b  c       p         o  o\n",
      "       m    m    a   l    t       e         a   t   a   o  s    a   i  p   s   p\n",
      "   o  a   a g         a  g     a   s    t  t   z    z    z    z    z    z    z  \n",
      "       h     a  a w    v     d o   n    e     s     m d o   e     z    z    z   \n",
      "  f   s      f    d        o  h      w     d       a   l            a   g       \n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3100: 1.619985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3200: 1.607259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3300: 1.617139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3400: 1.606629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.638431 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.615419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3700: 1.619682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3800: 1.621232 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3900: 1.623022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4000: 1.598891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "  i  t   f         n      v    i  n       m   a  h       r   e      i           \n",
      "         s      w     a d          i  t   f   g          s s       t   t     t  \n",
      "   r        o   b       a  f    t   h      e  i  d      s      o  p      o      \n",
      "   o   b            t  c       e        i      s c             w    a   p       \n",
      "       t   z    w    c           o  s    o  t       t  o      m          i  s   \n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4100: 1.582557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4200: 1.573374 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4300: 1.577703 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4400: 1.575359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4500: 1.596523 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.577643 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.573007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4800: 1.555542 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.568667 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5000: 1.567043 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "  k        t    w     l     l           o      f       s         w       d      \n",
      "   t  n      h     a  d      a     a   b          t        m    a   b    s    r \n",
      "    p     s      b  p    w     t    i      c         t       t    s         p   \n",
      "   d p       w   t     n    d    c       w    l       w   o   n    f    a       \n",
      "  a   w    t         w   r        f    l   t  t   i     t    m     a         a  \n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5100: 1.533466 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5200: 1.535235 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.536664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.536265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.526119 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.507587 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5700: 1.515342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5800: 1.542488 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.520195 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.525463 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "h       u        b         a   b o   g        a f     q t   n    o   n    f    f\n",
      "  t    a         t  e       t   s       h   h       i      o   e     s     w    \n",
      "    f    w     a   j      r  h         i  f     r         s a   s       o  t   p\n",
      "  s     b     s      r     c      a c             f    t   m    t   b     f     \n",
      "   w      w     t   e       o  t     t     t   o   n    n    f    f    t     p  \n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6100: 1.517994 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.528417 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.524813 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.516251 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.84\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.488586 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.535521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.506591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.513975 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6900: 1.516454 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000: 1.524973 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "       o  t   r       f   a g   a      o   n    s   w   a         a   r    o   n\n",
      "        m          c       a  a        i  i  a p        o  t   g       f       a\n",
      "      s    n          a   o   p         m   t   m     h    l     r       i  p   \n",
      " o        i   a s    t   y      o  t   f      c           t   s     t       t   \n",
      "        s      k       f     u   h     a  o  f        b       c     t    u   o  \n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "CPU times: user 17min 49s, sys: 1min 5s, total: 18min 54s\n",
      "Wall time: 6min 34s\n"
     ]
    }
   ],
   "source": [
    "%time graphExec(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "TODO 2\n",
    "---------\n",
    "\n",
    "Add a GRU cell instead of an LSTM cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "TODO 3\n",
    "---------\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(*chars):\n",
    "    for i, char in enumerate(chars):\n",
    "        print(\"char is: %s\" %char)\n",
    "        print(\"iti is: %d\" %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char is: a\n",
      "iti is: 0\n",
      "char is: a\n",
      "iti is: 1\n",
      "char is: b\n",
      "iti is: 2\n",
      "char is: c\n",
      "iti is: 3\n"
     ]
    }
   ],
   "source": [
    "test('a', 'a', 'b', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(*args):\n",
    "    formatString = ''\n",
    "    for i in range(len(args)):\n",
    "        if formatString == '': formatString = '{%d}'%i\n",
    "        else: formatString = formatString + ',{%d}'%i\n",
    "    formatString = '('+ formatString + ')'\n",
    "    return formatString.format(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,2,3)\n"
     ]
    }
   ],
   "source": [
    "print(test(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
