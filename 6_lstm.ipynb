{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Machine Learning and Deep Learning with Tensorflow\n",
    "=============\n",
    "\n",
    "Long Short Term Memory (LSTM) RNN model\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is 27:\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(\"vocabulary size is %d:\" %vocab_size)\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ' or char == None:\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0 and dictid < 27:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "              return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vSize):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vSize], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution(vSize):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vSize])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:   \n",
    "    # input: input gate (ix), forget gate (fx), memory cell (cx), output gate (ox)\n",
    "    X_ifco = tf.Variable(tf.truncated_normal([vocab_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # previous output: input gate (im), forget gate (fm), memory cell (cm), output gate (om)\n",
    "    M_ifco = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    # bias: input gate (ib), forget gate (if), memory cell (cb), output gate (ob)\n",
    "    B_ifco = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        \"\"\"\n",
    "        all_matmul =  tf.matmul(i, X_ifco) + tf.matmul(o, M_ifco) + B_ifco\n",
    "        input_matmul, forget_matmul, update, output_matmul = tf.split(1, 4, all_matmul)\n",
    "        all_gates = tf.sigmoid(all_matmul)\n",
    "        input_gate, forget_gate, _, output_gate = tf.split(1, 4, all_gates)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocab_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocab_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def graphExec(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels))))\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution(vocab_size), vocab_size)\n",
    "                        sentence = characters(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction, vocab_size)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298486 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "usvr  osqy  oirfbqvfnl hchrth iis hefuse aslkvwxrmxn tcvmsgorzmtu yueqjltrmvhrau\n",
      "bltaozqqrcp yaqeb d qprgvkltojumvbvfbfy hoeejgepzvt o gwdiosueuanhilvetttjbiesal\n",
      "owiwaylxukcbvg r aspyrpepwzksqxsiskn vrhahlriketowrurjslpantiqezcvnjekzpaira  e \n",
      "yawb psgpy lzqauc fz toq  to re rx ecqz la tiwadnvemp eefltante xcjt k wnpeljbpc\n",
      "gz jkrrgxs xvehw wcnzizty  rgjk ynvnbzsongzyxhucieszegvnic  xnd a dlhktmkfevatir\n",
      "================================================================================\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 100: 2.599799 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.13\n",
      "Validation set perplexity: 10.81\n",
      "Average loss at step 200: 2.253537 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 300: 2.099551 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 400: 1.998167 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 500: 1.937039 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 600: 1.910315 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.853282 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 800: 1.817695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 900: 1.829413 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1000: 1.823902 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "================================================================================\n",
      "king afucisp attle soffray compred by tover servaling sho the beer compuans one \n",
      "yow in a seed vistorate duinagh deriscopthirs riccramose the reath his k hamier \n",
      "gens most to the unferver the deary unry radnion is weathear berty dilled in to \n",
      "word iban chirtucary c sopoct diles akwed tran cicne to cli nomay yeartracsix an\n",
      "bate forksting id bevicmlovudig in eovaduicariss the seevers one hivh c they an \n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.775798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1200: 1.754837 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.730460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1400: 1.750088 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.740615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.747843 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1700: 1.710959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.677731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1900: 1.646796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.695079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "reast and tosiis election toge proper tin one nine nine four the follown for his\n",
      "fite st noter agens the can  ergenibualt sot evol they mudiom such it creal seko\n",
      "fict an exgerning to the crainmines blannad and for accust supprong thou propras\n",
      "tberrian to internation officious or popn this six feady hefdinish concent liffu\n",
      "pinctions contion conoftions anceslexp was inalle eng goali ia explibeal est to \n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.687011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2200: 1.681738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2300: 1.638839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2400: 1.660796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.678707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2600: 1.651518 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2700: 1.654980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.650776 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.648818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3000: 1.647464 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "why clession one nine six jedse recoldoing irenany has one in ne was of a collfa\n",
      "won ankestiveral contaous theyguging a the lassed the cobiting sharal was in her\n",
      "y the espaced in experiency seven eight one zero councifisu and solity in a ribe\n",
      "x ages to archyings inted britate how is wherea the become damed one theoply ing\n",
      "gual hadlm up time decand to weinny and mind throut one four de the inducatives \n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.632161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3200: 1.645097 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.640411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.671678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3500: 1.654933 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3600: 1.664794 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.650836 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.640213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.641982 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.648107 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "quiter suspervac poneperally senting in alrelmais archoress in a press roals azg\n",
      "e these the world including us by she dealis seven fivenswing pardical enrismany\n",
      "fols way on the ellangetbs hage add adlantly abromesse educal have when of reduc\n",
      "ilm not marsome of the has bassing the compleotaki and lost forfall sorn bitjest\n",
      "atticate of the canjusion au water game pelibed to fils the tombalspopble or int\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100: 1.631798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.633377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4300: 1.613636 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4400: 1.609051 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.616882 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.615025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4700: 1.622855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4800: 1.633212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4900: 1.632430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5000: 1.608292 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "exons seev bssidic that the was will propesses not five two five brag as ono of \n",
      "est produc in mille ckncaarism skarg while the play breminioncicin resplistics f\n",
      "nks govern on disliving bavol goldinific maugst of not the profiction mimin mef \n",
      "knoth of sladic mory the countimed reab s war eno portacties maintrain profumifi\n",
      "king take linten ancest lik sea twentury extantes pielt were is have sange publi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100: 1.604061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.589603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5300: 1.580704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400: 1.577647 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5500: 1.565895 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5600: 1.582594 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5700: 1.571373 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5800: 1.581856 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.574140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6000: 1.548082 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "f actisive in the rofetall masside company s sothon miggly granom tytery rinctor\n",
      "poccans referent about bleane officially andsibit amperors its prebibits and pet\n",
      "ching no schotion and will homan tr and rules lamicaging states he forchere with\n",
      "pp made the law gubok flows mient and charred juse on notay v schoal orizonst hi\n",
      "h in a his hars holt powerls other ghalosa semains way and lalt languary entiref\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6100: 1.563090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.539599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.542349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.542346 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6500: 1.560011 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.595723 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.578300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.605067 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.580907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.579284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      " imbroto drugi and is imburk know governmeno studies idented letter and used and\n",
      "n modernioous in a popular to this followed inald descripon red provider in one \n",
      "re approper were correactifian while city if a like recentle dotoky the complex \n",
      "yli sangoust brougp kfnearoment cainally eschaaised made were ore thingd genefog\n",
      "an many by the mallikercal fourdated re cult a congutily and the been albectivel\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "CPU times: user 4min 49s, sys: 41.9 s, total: 5min 31s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%time graphExec(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "TODO 1\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "ngram = 2\n",
    "\n",
    "class nGramBatchGenerator(object):\n",
    "    def __init__(self, ngram, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self.ngram = ngram\n",
    "        self.vocab_size = int(math.pow(vocab_size, ngram))\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)] # vector of the batch_size cursors positions in text\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = list()\n",
    "        for b in range(self._batch_size):\n",
    "            # getting the ids for ngram chars\n",
    "            j = 0\n",
    "            nCharId = 0\n",
    "            for i in range(self.ngram):\n",
    "                pos = self._cursor[b] + i\n",
    "                char = self._text[pos] if pos < self._text_size else ' '\n",
    "                charId = char2id(char)\n",
    "                if (charId == 0): continue\n",
    "                else: \n",
    "                    nCharId = nCharId + charId*int(math.pow(vocab_size, ngram-i-1))\n",
    "                    j = j + 1\n",
    "            batch.append(nCharId)\n",
    "            self._cursor[b] = (self._cursor[b] + self.ngram - 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones. \"\"\" \n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def nGramCharacters(probabilities, ngram):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [formatCharacter(c, ngram)\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def formatCharacter(c, ngram): \n",
    "    formatString = ''\n",
    "    args = []\n",
    "    for i in range(ngram):\n",
    "        idn = (c//int(math.pow(vocab_size, ngram-i-1))) % vocab_size\n",
    "        args.append(id2char(idn))\n",
    "        formatString = formatString + '{%d}'%i\n",
    "    return formatString.format(*args)\n",
    "\n",
    "\n",
    "train_batches = nGramBatchGenerator(ngram, train_text, batch_size, num_unrollings)\n",
    "valid_batches = nGramBatchGenerator(ngram, valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "n_vocab_size = int(math.pow(vocab_size, ngram))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:   \n",
    "    # input: input gate (ix), forget gate (fx), memory cell (cx), output gate (ox)\n",
    "    X_ifco = tf.Variable(tf.truncated_normal([n_vocab_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    # previous output: input gate (im), forget gate (fm), memory cell (cm), output gate (om)\n",
    "    M_ifco = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    # bias: input gate (ib), forget gate (if), memory cell (cb), output gate (ob)\n",
    "    B_ifco = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, n_vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([n_vocab_size]))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state, keep_prob):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        \"\"\"\n",
    "        xEmbedding = tf.nn.embedding_lookup(X_ifco, i)\n",
    "        xEmbeddingDrop = tf.nn.dropout(xEmbedding, keep_prob)\n",
    "        all_matmul = xEmbeddingDrop + tf.matmul(o, M_ifco) + B_ifco\n",
    "        input_matmul, forget_matmul, update, output_matmul = tf.split(1, 4, all_matmul)\n",
    "        all_gates = tf.sigmoid(all_matmul)\n",
    "        input_gate, forget_gate, _, output_gate = tf.split(1, 4, all_gates)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.int64, \n",
    "                         shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, keep_prob)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, \n",
    "                                            saved_sample_state, keep_prob=1) # for validation\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "prob = 0.5 #for drop-out\n",
    "\n",
    "def graphExec(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            feed_dict[keep_prob] = prob\n",
    "            _, l, predictions, lr = session.run([optimizer, loss, train_prediction, \n",
    "                                                 learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                # create sparse embedding one-hot encoding\n",
    "                embeddedLabels = np.zeros(predictions.shape)\n",
    "                for i, j in enumerate(labels):\n",
    "                    embeddedLabels[i, j] = 1.0   \n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, embeddedLabels))))\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution(n_vocab_size), n_vocab_size)\n",
    "                        sentence = characters(feed)[0]\n",
    "                        feed = [np.argmax(feed)]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction, n_vocab_size)\n",
    "                            sentence += characters(feed)[0]\n",
    "                            feed = [np.argmax(feed)]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    labels = np.zeros((1, n_vocab_size))\n",
    "                    labels[0, b[1]] = 1.0\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591077 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.57\n",
      "================================================================================\n",
      "       g                  w                                 q                   \n",
      "w                                                                               \n",
      "                                                            ab     n            \n",
      "                                               n              m               g \n",
      "e                                  d                        t                   \n",
      "================================================================================\n",
      "Validation set perplexity: 673.96\n",
      "Average loss at step 100: 5.343458 learning rate: 10.000000\n",
      "Minibatch perplexity: 160.19\n",
      "Validation set perplexity: 136.39\n",
      "Average loss at step 200: 4.200605 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.34\n",
      "Validation set perplexity: 28.56\n",
      "Average loss at step 300: 3.173527 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.09\n",
      "Validation set perplexity: 13.75\n",
      "Average loss at step 400: 2.651559 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.48\n",
      "Validation set perplexity: 10.74\n",
      "Average loss at step 500: 2.431159 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.81\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 600: 2.310958 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 700: 2.197575 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.08\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 800: 2.123286 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 900: 2.121164 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1000: 2.100513 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "================================================================================\n",
      "  o  t  d          d    b      c     a   s  i              i  s             a w \n",
      "s a   p       a  d   t    t   a     s      t   f    f    s   g    i      w   b  \n",
      " l    n      a     c         t   b     w   r        h    a   r         t  t   o \n",
      "    a c    s   e     o   e    f   r      t  a c      t   h   d   a t   c     h  \n",
      "  o      b     n        w    n   a   c       e    s    p       t   t   s        \n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 1100: 2.032997 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 1200: 1.983099 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1300: 1.959389 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1400: 1.972972 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1500: 1.968896 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1600: 1.966524 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1700: 1.927117 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1800: 1.880923 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1900: 1.844119 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 2000: 1.897402 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "================================================================================\n",
      "   a   p            j      b  f    v     t            s       s     s     w     \n",
      "        i         t   r        h   c     i  h       d       o   n      f    a   \n",
      "    a     f    c           p   c      p       a   l    n    n    n    s     s   \n",
      "    g     g    l     b     i  l   c        i      b       s     c     m     o  a\n",
      "   o  p         w   c         f        b    g  s     a        a r      o  a s d \n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 2100: 1.893504 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2200: 1.877211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 2300: 1.832030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2400: 1.845059 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2500: 1.859989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2600: 1.837730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2700: 1.840645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2800: 1.826949 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2900: 1.817480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3000: 1.819892 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "================================================================================\n",
      "        a      b        f   a s   s       i       r      u            a p       \n",
      "   i  s       o  t   p          t     b     c       p         p         o   n   \n",
      "    b  n  e        a     h   n          f    s f    a   i  e      i  o    c     \n",
      "x t      o   f    t      j         w    b   p        t    l         f      i  o \n",
      "       u       t         a   u        s i          t   m        f   i  p     d  \n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3100: 1.795532 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3200: 1.815727 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3300: 1.800536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3400: 1.837047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3500: 1.815992 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3600: 1.829496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3700: 1.803374 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3800: 1.786283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3900: 1.786209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.798697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "      a        d      g      a  u     j         e       o    a  e         a   m \n",
      "      g    b  m        d  g       s    a   p      h        u    o   s     s   s \n",
      "     p         n   a s         a   w   h  m         z   s     z    z    t     s \n",
      " a         a  e        h      o   s     e        o  t   h    a      o  t   m   s\n",
      "     d   n   b        e   t   w   r          e            t   i   q    w    a   \n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4100: 1.772883 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4200: 1.777889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4300: 1.760055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4400: 1.751518 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.754981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4600: 1.757698 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4700: 1.768856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4800: 1.769682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4900: 1.760284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5000: 1.732852 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "   t    f       a         s     t         b      f     h    e      t    o  p    \n",
      "     m       f    c      h      t   e    h   t   z    z    z    z    t     t   r\n",
      "   f    t   w    o  t   b      u      a s              a   w    s     t   c     \n",
      "  r          d      c        t   f    b      b       a   t   t   m    n    t    \n",
      "      h   r    t       a   c       h      b           o  d     a   t      h   p \n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.727949 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.721516 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.711646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.705293 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5500: 1.700060 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.711092 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5700: 1.705007 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5800: 1.706339 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5900: 1.702687 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6000: 1.672968 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "            l  p         i  a        t    s       c       s     o   n    a   c  \n",
      "     t  b  r       t   i  i           o  t   p        p     a  t   l      b     \n",
      "            a   f    e         o  w    n    n    f      s  s         p       s  \n",
      " y i  o  l    p        a m      b  o      o  t   i    h    f    e       h   a   \n",
      "  c        o  t   g     f       o  h    n    s   e     f    t     s    a   o  a \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6100: 1.686615 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.664865 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6300: 1.672334 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.666162 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6500: 1.682798 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6600: 1.717515 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.700447 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6800: 1.727876 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6900: 1.704515 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7000: 1.699086 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "================================================================================\n",
      "       t   p      i  a     o  c      f       a   i   c   i          t   c       \n",
      "         b  i   h         t   t       i  o   n    f    z    z    z    z    z    \n",
      "    w     l     a p        h     i     r      d         i  t   p    s     c     \n",
      "  o  p            p       i  a       i   k     a   a p    c              d   p  \n",
      "        s   f  m      t     a  i  f    t   o          o    t   n    t  i  h   r \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "CPU times: user 18min 3s, sys: 1min 6s, total: 19min 9s\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%time graphExec(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "TODO 2\n",
    "---------\n",
    "\n",
    "Add a GRU cell instead of an LSTM cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "TODO 3\n",
    "---------\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I copied the below code (Seq2SeqModel class) from the TensorFlow [github repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py) as to not reinvent the wheel and keep a today's version of the Seq2SeqModel class in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Sequence-to-sequence model with an attention mechanism.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "\n",
    "\n",
    "class Seq2SeqModel(object):\n",
    "    \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n",
    "\n",
    "    This class implements a multi-layer recurrent neural network as encoder,\n",
    "    and an attention-based decoder. This is the same as the model described in\n",
    "    this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n",
    "    or into the seq2seq library for complete model implementation.\n",
    "    This class also allows to use GRU cells in addition to LSTM cells, and\n",
    "    sampled softmax to handle large output vocabulary size. A single-layer\n",
    "    version of this model, but with bi-directional encoder, was presented in\n",
    "    http://arxiv.org/abs/1409.0473\n",
    "    and sampled softmax is described in Section 3 of the following paper.\n",
    "    http://arxiv.org/abs/1412.2007\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               source_vocab_size,\n",
    "               target_vocab_size,\n",
    "               buckets,\n",
    "               size,\n",
    "               num_layers,\n",
    "               max_gradient_norm,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               learning_rate_decay_factor,\n",
    "               use_lstm=False,\n",
    "               num_samples=512,\n",
    "               forward_only=False,\n",
    "               dtype=tf.float32):\n",
    "        \"\"\"Create the model.\n",
    "\n",
    "        Args:\n",
    "          source_vocab_size: size of the source vocabulary.\n",
    "          target_vocab_size: size of the target vocabulary.\n",
    "          buckets: a list of pairs (I, O), where I specifies maximum input length\n",
    "            that will be processed in that bucket, and O specifies maximum output\n",
    "            length. Training instances that have inputs longer than I or outputs\n",
    "            longer than O will be pushed to the next bucket and padded accordingly.\n",
    "            We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n",
    "          size: number of units in each layer of the model.\n",
    "          num_layers: number of layers in the model.\n",
    "          max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "          batch_size: the size of the batches used during training;\n",
    "            the model construction is independent of batch_size, so it can be\n",
    "            changed after initialization if this is convenient, e.g., for decoding.\n",
    "          learning_rate: learning rate to start with.\n",
    "          learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "          use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
    "          num_samples: number of samples for sampled softmax.\n",
    "          forward_only: if set, we do not construct the backward pass in the model.\n",
    "          dtype: the data type to use to store internal variables.\n",
    "        \"\"\"\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.buckets = buckets\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(\n",
    "            float(learning_rate), trainable=False, dtype=dtype)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # If we use sampled softmax, we need an output projection.\n",
    "        output_projection = None\n",
    "        softmax_loss_function = None\n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if num_samples > 0 and num_samples < self.target_vocab_size:\n",
    "            w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n",
    "            w = tf.transpose(w_t)\n",
    "            b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n",
    "            output_projection = (w, b)\n",
    "\n",
    "            def sampled_loss(inputs, labels):\n",
    "                labels = tf.reshape(labels, [-1, 1])\n",
    "                # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "                # avoid numerical instabilities.\n",
    "                local_w_t = tf.cast(w_t, tf.float32)\n",
    "                local_b = tf.cast(b, tf.float32)\n",
    "                local_inputs = tf.cast(inputs, tf.float32)\n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                               num_samples, self.target_vocab_size), dtype)\n",
    "            softmax_loss_function = sampled_loss\n",
    "\n",
    "        # Create the internal multi-layer cell for our RNN.\n",
    "        with tf.variable_scope(\"embedding_attention_seq2seq\", reuse=True):\n",
    "            single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "            if use_lstm:\n",
    "                with tf.variable_scope(\"forward\", reuse=True):\n",
    "                    single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "                with tf.variable_scope(\"other\", reuse=True):\n",
    "                    single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "            cell = single_cell\n",
    "            if num_layers > 1:\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "        # The seq2seq function: we use embedding for the input and attention.\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            return tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "              encoder_inputs,\n",
    "              decoder_inputs,\n",
    "              cell,\n",
    "              num_encoder_symbols=source_vocab_size,\n",
    "              num_decoder_symbols=target_vocab_size,\n",
    "              embedding_size=size,\n",
    "              output_projection=output_projection,\n",
    "              feed_previous=do_decode,\n",
    "              dtype=dtype)\n",
    "\n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                    name=\"encoder{0}\".format(i)))\n",
    "        for i in xrange(buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                    name=\"decoder{0}\".format(i)))\n",
    "            self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                    name=\"weight{0}\".format(i)))\n",
    "\n",
    "        # Our targets are decoder inputs shifted by one.\n",
    "        targets = [self.decoder_inputs[i + 1]\n",
    "                   for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "        # Training outputs and losses.\n",
    "        if forward_only:\n",
    "            self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets, \n",
    "                lambda x, y: seq2seq_f(x, y, True),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "            # If we use output projection, we need to project outputs for decoding.\n",
    "            if output_projection is not None:\n",
    "                for b in xrange(len(buckets)):\n",
    "                    self.outputs[b] = [\n",
    "                      tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "                      for output in self.outputs[b]\n",
    "                    ]\n",
    "        else:\n",
    "            self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets,\n",
    "                lambda x, y: seq2seq_f(x, y, False),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "        # Gradients and SGD update operation for training the model.\n",
    "        params = tf.trainable_variables()\n",
    "        if not forward_only:\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            for b in xrange(len(buckets)):\n",
    "                gradients = tf.gradients(self.losses[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                                 max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.updates.append(opt.apply_gradients(\n",
    "                    zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "        \"\"\"Run a step of the model feeding the given inputs.\n",
    "\n",
    "        Args:\n",
    "          session: tensorflow session to use.\n",
    "          encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
    "          decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
    "          target_weights: list of numpy float vectors to feed as target weights.\n",
    "          bucket_id: which bucket of the model to use.\n",
    "          forward_only: whether to do the backward step or only forward.\n",
    "\n",
    "        Returns:\n",
    "          A triple consisting of gradient norm (or None if we did not do backward),\n",
    "          average perplexity, and the outputs.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: if length of encoder_inputs, decoder_inputs, or\n",
    "            target_weights disagrees with bucket size for the specified bucket_id.\n",
    "        \"\"\"\n",
    "        # Check if the sizes match.\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        if len(encoder_inputs) != encoder_size:\n",
    "            raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                           \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "        if len(decoder_inputs) != decoder_size:\n",
    "            raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                           \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "        if len(target_weights) != decoder_size:\n",
    "            raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                           \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {}\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "        # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not forward_only:\n",
    "            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                           self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                           self.losses[bucket_id]]  # Loss for this batch.\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "            for l in xrange(decoder_size):  # Output logits.\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "            return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "            return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "    def get_batch(self, data, bucket_id):\n",
    "        \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n",
    "\n",
    "        To feed data in step(..) it must be a list of batch-major vectors, while\n",
    "        data here contains single length-major cases. So the main logic of this\n",
    "        function is to re-index data cases to be in the proper format for feeding.\n",
    "\n",
    "        Args:\n",
    "          data: a tuple of size len(self.buckets) in which each element contains\n",
    "            lists of pairs of input and output data that we use to create a batch.\n",
    "          bucket_id: integer, which bucket to get the batch for.\n",
    "\n",
    "        Returns:\n",
    "          The triple (encoder_inputs, decoder_inputs, target_weights) for\n",
    "          the constructed batch that has the proper format to call step(...) later.\n",
    "        \"\"\"\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "        # Get a random batch of encoder and decoder inputs from data,\n",
    "        # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "        for _ in xrange(self.batch_size):\n",
    "            encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "            # Encoder inputs are padded and then reversed.\n",
    "            encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "            # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "            decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "            decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                                [data_utils.PAD_ID] * decoder_pad_size)\n",
    "\n",
    "        # Now we create batch-major vectors from the data selected above.\n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "        # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "        for length_idx in xrange(encoder_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                        for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "        # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "        for length_idx in xrange(decoder_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                        for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "            # Create target_weights to be 0 for targets that are padding.\n",
    "            batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "            for batch_idx in xrange(self.batch_size):\n",
    "                # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "                # The corresponding target is decoder_input shifted by 1 forward.\n",
    "                if length_idx < decoder_size - 1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model, I have leveraged was already done to [translate sentences](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py) for english to french."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    return word.strip(' ')\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "    correct = 0\n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32)\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32)\n",
    "    target_weights[0,:] = 1.0\n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_)\n",
    "    for i in xrange(word_size + 1):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "        p = np.argmax(output_logits[i], axis=1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "\n",
    "    print(decoder_inputs)\n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '->', output_word, '({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    return correct\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    noOfWords = (len(words) / batch_size) * batch_size\n",
    "    correct = 0\n",
    "    for i in xrange(noOfWords // batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "         \n",
    "        encoder_inputs = [np.zeros(noOfWords + 1, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "        for i, range_words in enumerate(range_words):\n",
    "            for j, c in enumerate(range_words):\n",
    "                encoder_inputs[i][j] = char2id(c) \n",
    "        encoder_inputs = np.transpose(encoder_inputs)\n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, noOfWords, (float(correct) / noOfWords) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "num_steps = 7001\n",
    "noOfWords = 5\n",
    "\n",
    "def graphExec(graph):\n",
    "    with tf.Session() as session:\n",
    "        seq2seq_model = Seq2SeqModel(source_vocab_size=vocab_size,\n",
    "                                    target_vocab_size=vocab_size,\n",
    "                                    buckets=[(noOfWords + 1, noOfWords + 2)],\n",
    "                                    size=num_nodes,\n",
    "                                    num_layers=3,\n",
    "                                    max_gradient_norm=5.0,\n",
    "                                    batch_size=batch_size,\n",
    "                                    learning_rate=0.5,\n",
    "                                    learning_rate_decay_factor=0.99,\n",
    "                                    use_lstm=True,\n",
    "                                    forward_only=False)\n",
    "\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in xrange(num_steps):\n",
    "            encoder_inputs = [np.random.randint(1, vocab_size, noOfWords + 1) for _ in xrange(batch_size)]\n",
    "            decoder_inputs = [np.zeros(noOfWords + 2, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "            weights = [np.ones(noOfWords + 2, dtype=np.float32) for _ in xrange(batch_size)]\n",
    "            for i in xrange(batch_size):\n",
    "                r = random.randint(1, noOfWords)\n",
    "                encoder_inputs[i][r:] = 0\n",
    "                decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "                weights[i][r+1:] = 0.0\n",
    "\n",
    "            encoder_inputs = np.transpose(encoder_inputs)\n",
    "            decoder_inputs = np.transpose(decoder_inputs)\n",
    "            weights = np.transpose(weights)\n",
    "\n",
    "            _, loss, _ = seq2seq_model.step(session, encoder_inputs, decoder_inputs, \n",
    "                                            weights, 0, False)\n",
    "            if step % 1000 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(\"the quick brown fox\", seq2seq_model, session)\n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(\"the quick brown fox\", seq2seq_model, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x11a21a590>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x11dc91750>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-99-685a8d43695e>\", line 116, in seq2seq_f\n    dtype=dtype)\n  File \"<ipython-input-99-685a8d43695e>\", line 152, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"<ipython-input-99-685a8d43695e>\", line 153, in __init__\n    softmax_loss_function=softmax_loss_function)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-556e44d0af1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time programExec()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-148-56fda4c2d673>\u001b[0m in \u001b[0;36mprogramExec\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                     \u001b[0mlearning_rate_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                     \u001b[0muse_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                     forward_only=False)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-cb28120de380>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, num_samples, forward_only, dtype)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq2seq_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 softmax_loss_function=softmax_loss_function)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Gradients and SGD update operation for training the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36mmodel_with_buckets\u001b[0;34m(encoder_inputs, decoder_inputs, targets, weights, buckets, seq2seq, softmax_loss_function, per_example_loss, name)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                          reuse=True if j > 0 else None):\n\u001b[1;32m   1029\u001b[0m         bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[0]],\n\u001b[0;32m-> 1030\u001b[0;31m                                     decoder_inputs[:bucket[1]])\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mper_example_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-cb28120de380>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq2seq_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 softmax_loss_function=softmax_loss_function)\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-cb28120de380>\u001b[0m in \u001b[0;36mseq2seq_f\u001b[0;34m(encoder_inputs, decoder_inputs, do_decode)\u001b[0m\n\u001b[1;32m    118\u001b[0m               \u001b[0moutput_projection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m               \u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_decode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m               dtype=dtype)\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Feeds for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36membedding_attention_seq2seq\u001b[0;34m(encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, num_heads, output_projection, feed_previous, dtype, scope, initial_state_attention)\u001b[0m\n\u001b[1;32m    747\u001b[0m         embedding_size=embedding_size)\n\u001b[1;32m    748\u001b[0m     encoder_outputs, encoder_state = rnn.rnn(\n\u001b[0;32m--> 749\u001b[0;31m         encoder_cell, encoder_inputs, dtype=dtype)\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# First calculate a concatenation of encoder outputs to put attention on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    217\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m    218\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             dtype=data_type)\n\u001b[0m\u001b[1;32m    752\u001b[0m         embedded = embedding_ops.embedding_lookup(\n\u001b[1;32m    753\u001b[0m             embedding, array_ops.reshape(inputs, [-1]))\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    871\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    698\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    215\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    200\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonat/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    492\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 494\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-99-685a8d43695e>\", line 116, in seq2seq_f\n    dtype=dtype)\n  File \"<ipython-input-99-685a8d43695e>\", line 152, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"<ipython-input-99-685a8d43695e>\", line 153, in __init__\n    softmax_loss_function=softmax_loss_function)\n"
     ]
    }
   ],
   "source": [
    "%time graphExec(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
